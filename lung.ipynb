{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    CropForegroundd,\n",
    "    LoadImaged,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandShiftIntensityd,\n",
    "    ScaleIntensityRanged,\n",
    "    Spacingd,\n",
    "    RandRotate90d,\n",
    "    EnsureTyped,\n",
    ")\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SwinUNETR\n",
    "\n",
    "from monai.data import (\n",
    "    ThreadDataLoader,\n",
    "    CacheDataset,\n",
    "    load_decathlon_datalist,\n",
    "    decollate_batch,\n",
    "    set_track_meta,\n",
    ")\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root dir is: /tmp/tmpgxgypslf\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"./data/\")\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(f\"root dir is: {root_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 4\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.croppad.dictionary CropForegroundd.__init__:allow_smaller: Current default value of argument `allow_smaller=True` has been deprecated since version 1.2. It will be changed to `allow_smaller=False` in version 1.5.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=False),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(96, 96, 96),\n",
    "            pos=1,\n",
    "            neg=1,\n",
    "            num_samples=num_samples,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=0,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[0],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[1],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandFlipd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            spatial_axis=[2],\n",
    "            prob=0.10,\n",
    "        ),\n",
    "        RandRotate90d(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            prob=0.10,\n",
    "            max_k=3,\n",
    "        ),\n",
    "        RandShiftIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            offsets=0.10,\n",
    "            prob=0.50,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        CropForegroundd(keys=[\"image\", \"label\"], source_key=\"image\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.5, 1.5, 2.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"], device=device, track_meta=True),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading dataset:   0%|          | 0/24 [00:00<?, ?it/s]axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (436, 228), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (399, 242), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (469, 251), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (426, 251), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (484, 276), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (451, 271), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (420, 289), channels = 512,please make sure the input is in the channel-first format.\n",
      "Loading dataset:   8%|▊         | 2/24 [00:04<00:44,  2.03s/it]axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (420, 125), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (390, 423), channels = 512,please make sure the input is in the channel-first format.\n",
      "Loading dataset:  12%|█▎        | 3/24 [00:06<00:42,  2.01s/it]axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (406, 218), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (393, 211), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (431, 227), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (446, 241), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (444, 112), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (393, 252), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (421, 296), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (414, 493), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (418, 245), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (400, 291), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (410, 277), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (428, 636), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (488, 248), channels = 512,please make sure the input is in the channel-first format.\n",
      "Loading dataset:  38%|███▊      | 9/24 [00:13<00:20,  1.34s/it]axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (386, 450), channels = 512,please make sure the input is in the channel-first format.\n",
      "Loading dataset:  83%|████████▎ | 20/24 [00:14<00:01,  2.04it/s]axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (419, 304), channels = 512,please make sure the input is in the channel-first format.\n",
      "Loading dataset: 100%|██████████| 24/24 [00:15<00:00,  1.57it/s]\n",
      "Loading dataset:   0%|          | 0/6 [00:00<?, ?it/s]axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (435, 240), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (471, 242), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (412, 252), channels = 512,please make sure the input is in the channel-first format.\n",
      "Loading dataset:  17%|█▋        | 1/6 [00:03<00:17,  3.47s/it]axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (434, 483), channels = 512,please make sure the input is in the channel-first format.\n",
      "Loading dataset:  33%|███▎      | 2/6 [00:05<00:10,  2.61s/it]axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (472, 567), channels = 512,please make sure the input is in the channel-first format.\n",
      "axcodes ('RAS') length is smaller than number of input spatial dimensions D=2.\n",
      "Orientation: spatial shape = (417, 589), channels = 512,please make sure the input is in the channel-first format.\n",
      "Loading dataset: 100%|██████████| 6/6 [00:09<00:00,  1.60s/it]\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"data/\"\n",
    "split_json = \"dataset.json\"\n",
    "\n",
    "datasets = data_dir + split_json\n",
    "datalist = load_decathlon_datalist(datasets, True, \"training\")\n",
    "val_files = load_decathlon_datalist(datasets, True, \"validation\")\n",
    "\n",
    "train_ds = CacheDataset(\n",
    "    data=datalist,\n",
    "    transform=train_transforms,\n",
    "    cache_num=24,\n",
    "    cache_rate=1.0,\n",
    "    num_workers=8,\n",
    ")\n",
    "\n",
    "train_loader = ThreadDataLoader(\n",
    "    train_ds,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_ds = CacheDataset(data=val_files, transform=val_transforms, cache_num=6, cache_rate=1.0, num_workers=4)\n",
    "val_loader = ThreadDataLoader(val_ds, num_workers=0, batch_size=32)\n",
    "\n",
    "set_track_meta(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: torch.Size([512, 244, 271]), label shape: torch.Size([512, 244, 271])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/erikmyhre/Develop/VI/vi_project/lung.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikmyhre/Develop/VI/vi_project/lung.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikmyhre/Develop/VI/vi_project/lung.ipynb#X14sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mimage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/erikmyhre/Develop/VI/vi_project/lung.ipynb#X14sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m plt\u001b[39m.\u001b[39mimshow(img[\u001b[39m0\u001b[39;49m, :, :, slice_map[img_name]]\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu(), cmap\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikmyhre/Develop/VI/vi_project/lung.ipynb#X14sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/erikmyhre/Develop/VI/vi_project/lung.ipynb#X14sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m plt\u001b[39m.\u001b[39mtitle(\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/monai/lib/python3.10/site-packages/monai/data/meta_tensor.py:282\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    281\u001b[0m     kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 282\u001b[0m ret \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m__torch_function__(func, types, args, kwargs)\n\u001b[1;32m    283\u001b[0m \u001b[39m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[39m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[39m#     return ret\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[39mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m~/miniconda3/envs/monai/lib/python3.10/site-packages/torch/_tensor.py:1386\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m   1385\u001b[0m \u001b[39mwith\u001b[39;00m _C\u001b[39m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1386\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1387\u001b[0m     \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1388\u001b[0m         \u001b[39mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 3"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "slice_map = {\n",
    "    \"lung_022.nii.gz\": 75,\n",
    "    \"lung_041.nii.gz\": 30,\n",
    "}\n",
    "case_num = 1\n",
    "img_name = os.path.split(val_ds[case_num][\"image\"].meta[\"filename_or_obj\"])[1]\n",
    "img = val_ds[case_num][\"image\"]\n",
    "label = val_ds[case_num][\"label\"]\n",
    "img_shape = img.shape\n",
    "label_shape = label.shape\n",
    "print(f\"image shape: {img_shape}, label shape: {label_shape}\")\n",
    "plt.figure(\"image\", (18, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(img[0, :, :, slice_map[img_name]].detach().cpu(), cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[0, :, :, slice_map[img_name]].detach().cpu())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SwinUNETR(\n",
    "    img_size=(96, 96, 96),\n",
    "    in_channels=1,\n",
    "    out_channels=14,\n",
    "    feature_size=48,\n",
    "    use_checkpoint=True,\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.losses import DiceCELoss\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "loss_function = DiceCELoss(to_onehot_y=True, softmax=True)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(epoch_iterator_val):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in epoch_iterator_val:\n",
    "            val_inputs, val_labels = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "            with torch.cuda.amp.autocast():\n",
    "                val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, model)\n",
    "            val_labels_list = decollate_batch(val_labels)\n",
    "            val_labels_convert = [post_label(val_label_tensor) for val_label_tensor in val_labels_list]\n",
    "            val_outputs_list = decollate_batch(val_outputs)\n",
    "            val_output_convert = [post_pred(val_pred_tensor) for val_pred_tensor in val_outputs_list]\n",
    "            dice_metric(y_pred=val_output_convert, y=val_labels_convert)\n",
    "            epoch_iterator_val.set_description(\"Validate (%d / %d Steps)\" % (global_step, 10.0))\n",
    "        mean_dice_val = dice_metric.aggregate().item()\n",
    "        dice_metric.reset()\n",
    "    return mean_dice_val\n",
    "\n",
    "\n",
    "def train(global_step, train_loader, dice_val_best, global_step_best):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    epoch_iterator = tqdm(train_loader, desc=\"Training (X / X Steps) (loss=X.X)\", dynamic_ncols=True)\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        step += 1\n",
    "        x, y = (batch[\"image\"].cuda(), batch[\"label\"].cuda())\n",
    "        with torch.cuda.amp.autocast():\n",
    "            logit_map = model(x)\n",
    "            loss = loss_function(logit_map, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        epoch_loss += loss.item()\n",
    "        scaler.unscale_(optimizer)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        epoch_iterator.set_description(f\"Training ({global_step} / {max_iterations} Steps) (loss={loss:2.5f})\")\n",
    "        if (global_step % eval_num == 0 and global_step != 0) or global_step == max_iterations:\n",
    "            epoch_iterator_val = tqdm(val_loader, desc=\"Validate (X / X Steps) (dice=X.X)\", dynamic_ncols=True)\n",
    "            dice_val = validation(epoch_iterator_val)\n",
    "            epoch_loss /= step\n",
    "            epoch_loss_values.append(epoch_loss)\n",
    "            metric_values.append(dice_val)\n",
    "            if dice_val > dice_val_best:\n",
    "                dice_val_best = dice_val\n",
    "                global_step_best = global_step\n",
    "                torch.save(model.state_dict(), os.path.join(root_dir, \"best_metric_model.pth\"))\n",
    "                print(\n",
    "                    \"Model Was Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(dice_val_best, dice_val)\n",
    "                )\n",
    "            else:\n",
    "                print(\n",
    "                    \"Model Was Not Saved ! Current Best Avg. Dice: {} Current Avg. Dice: {}\".format(\n",
    "                        dice_val_best, dice_val\n",
    "                    )\n",
    "                )\n",
    "        global_step += 1\n",
    "    return global_step, dice_val_best, global_step_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 100000\n",
    "eval_num = 500\n",
    "post_label = AsDiscrete(to_onehot=14)\n",
    "post_pred = AsDiscrete(argmax=True, to_onehot=14)\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\", get_not_nans=False)\n",
    "global_step = 0\n",
    "dice_val_best = 0.0\n",
    "global_step_best = 0\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "while global_step < max_iterations:\n",
    "    global_step, dice_val_best, global_step_best = train(global_step, train_loader, dice_val_best, global_step_best)\n",
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train completed, best_metric: {dice_val_best:.4f} \" f\"at iteration: {global_step_best}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Iteration Average Loss\")\n",
    "x = [eval_num * (i + 1) for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [eval_num * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_num = 4\n",
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    img_name = os.path.split(val_ds[case_num][\"image\"].meta[\"filename_or_obj\"])[1]\n",
    "    img = val_ds[case_num][\"image\"]\n",
    "    label = val_ds[case_num][\"label\"]\n",
    "    val_inputs = torch.unsqueeze(img, 1).cuda()\n",
    "    val_labels = torch.unsqueeze(label, 1).cuda()\n",
    "    val_outputs = sliding_window_inference(val_inputs, (96, 96, 96), 4, model, overlap=0.8)\n",
    "    plt.figure(\"check\", (18, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title(\"image\")\n",
    "    plt.imshow(val_inputs.cpu().numpy()[0, 0, :, :, slice_map[img_name]], cmap=\"gray\")\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.title(\"label\")\n",
    "    plt.imshow(val_labels.cpu().numpy()[0, 0, :, :, slice_map[img_name]])\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.title(\"output\")\n",
    "    plt.imshow(torch.argmax(val_outputs, dim=1).detach().cpu()[0, :, :, slice_map[img_name]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if directory is None:\n",
    "    shutil.rmtree(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "monai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
